1975 Pacific Hurricane Season - Methodology
Anastasia Drakou | a.drakou@hotmail.com | (+30)6941523553


Wikipedia served as the perfect starting point because of its wealth of information and well-structured format. To navigate this structure effectively, I chose BeautifulSoup, a popular library ideal for parsing static HTML content like Wikipedia pages. This streamlined the process of extracting key details like hurricane names, durations, and descriptions. Once the data was extracted, I used regular expressions and string manipulation techniques to remove any unwanted elements. This included things like citations and non-breaking spaces, ensuring a clean and organized dataset. To go beyond basic details and extract information like the number of deaths and affected areas, I leveraged pre-trained NLP models like BERT and DistilBERT. These powerful tools analyze the context of the text, allowing for a more accurate and flexible extraction process compared to simple string matching. After many tries with a lot of models these models had the best and more accurate performance. One improvement I aim to implement is refining the extraction of affected areas by including geographical references (e.g., “southwest of...”) to provide even more precise information.

To ensure the accuracy of the extracted data, I implemented several validation checks. These checks verified if the hurricane name it is truly existed (by using gpt-2 model), the proper date formats, ensured start dates came before end dates, and confirmed the validity of death counts as integers. Additionally, I eliminated duplicate locations for each hurricane within the list of affected areas.  Finally, to add another layer of validation, I cross-referenced locations with a map to verify their plausibility within the context of the 1975 Pacific hurricane season.

After cleaning and validating the data, the final step involved saving it into a convenient and widely accessible format: a CSV file.